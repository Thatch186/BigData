{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets Playbook \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def divide_dataset_by_column(dataset, column):\n",
    "    unique_values = dataset[column].unique()\n",
    "    divided_datasets = {}\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = pd.DataFrame(dataset[dataset[column] == value])\n",
    "        divided_datasets[value] = subset\n",
    "\n",
    "    return divided_datasets\n",
    "\n",
    "def fill_null_values(dataset, strategy):\n",
    "    # select only the columns with float data type\n",
    "    float_cols = dataset.select_dtypes(include=['float'])\n",
    "    \n",
    "    # create a SimpleImputer object with the mean strategy\n",
    "    \n",
    "    imp = SimpleImputer(strategy = 'median')\n",
    "\n",
    "    # fit the imputer to the float columns\n",
    "    imp.fit(float_cols)\n",
    "\n",
    "    # transform the float columns by replacing missing values with the strategy\n",
    "    not_null = imp.transform(float_cols)\n",
    "\n",
    "    # replace the original float columns with the transformed values\n",
    "    dataset[float_cols.columns] = not_null\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def fill_null_by_income(dataset, strategy):\n",
    "    income_datasets = divide_dataset_by_column(dataset, 'incomeLevel')\n",
    "    filled_datasets = []\n",
    "\n",
    "    for value, df in income_datasets.items():\n",
    "        filled_df = fill_null_values(df, strategy)\n",
    "        filled_datasets.append(filled_df)\n",
    "\n",
    "    filled_dataset = pd.concat(filled_datasets)\n",
    "\n",
    "    return filled_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wbgapi\n",
    "import wbgapi as wb\n",
    "\n",
    "economy = wb.economy.info()\n",
    "table_dict = vars(economy)\n",
    "df_economy = pd.DataFrame(table_dict.get('items'))\n",
    "\n",
    "filtered_dataset = df_economy.dropna(subset=['incomeLevel'])\n",
    "df_income = filtered_dataset[['id', 'incomeLevel']].copy()\n",
    "df_income.rename(columns={'id': 'Country Code'}, inplace=True)\n",
    "\n",
    "df_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_income['incomeLevel'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sm = pd.read_csv('social_media-ww-yearly-2009-2023.csv')\n",
    "df_sm = df_sm.rename(columns={'Date': 'Year'})\n",
    "df_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental Health Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls_file = pd.ExcelFile('Mental health Depression disorder Data.xlsx')\n",
    "page_list = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each page on the excel sheet:\n",
    "- Load to a dataframe\n",
    "- Filter out rows with year different from 'yyyy' \n",
    "- Add the dataframe to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_regex = re.compile(r'^\\d{4}$')\n",
    "\n",
    "for page_name in xls_file.sheet_names:\n",
    "\n",
    "    df = pd.read_excel(xls_file, page_name, engine='openpyxl')\n",
    "    valid_years_mask = df['Year'].astype(str).apply(lambda x: bool(year_regex.match(x)))\n",
    "    filtered_df = df[valid_years_mask]\n",
    "\n",
    "    page_list.append(filtered_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the general dataset\n",
    "- Merging the different dataframes from each page by the columns 'Entity', 'Year', 'Code'\n",
    "- Removing columns with all values empty\n",
    "- removing duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mh = page_list[0]\n",
    "for i in range(1, len(page_list)):\n",
    "    merged_mh = pd.merge(\n",
    "        merged_mh,\n",
    "        page_list[i],\n",
    "        on=['Entity', 'Year', 'Code'],\n",
    "        how='outer',\n",
    "        suffixes=('_left', '_right')\n",
    "    )\n",
    "\n",
    "merged_mh = merged_mh.rename(columns={'Entity': 'Country', 'Code': 'Country Code'})\n",
    "merged_mh = merged_mh.loc[:, ~merged_mh.columns.duplicated()]\n",
    "merged_mh = merged_mh.dropna(axis=1, how='all')\n",
    "print(merged_mh.isna().any())\n",
    "merged_mh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of all the columns except the first 3 to float\n",
    "merged_mh.iloc[:, 3:] = merged_mh.iloc[:, 3:].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Income Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_mh = pd.merge(merged_mh, df_income[['Country Code', 'incomeLevel']], on='Country Code', how='left')\n",
    "#merged_mh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_mh.shape)\n",
    "null_columns = merged_mh.columns[merged_mh.isnull().all()]\n",
    "\n",
    "# Step 2: Remove columns with all null values\n",
    "merged_mh = merged_mh.drop(null_columns, axis=1)\n",
    "print(merged_mh.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating missing values\n",
    "\n",
    "We use a simple inputer that fills the missing values on a collumn with the median value of that collumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_income = fill_null_values(merged_mh,'median')\n",
    "# check if any missing values remain in the float columns\n",
    "print(mh_income.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mh.to_csv('merged_health.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the resulting dataset to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use ParquetDataset to read in all of the files as a single dataset\n",
    "merged_mh.to_parquet('my_data.parquet.gzip', compression='gzip')\n",
    "\n",
    "parquet_mh_df = pd.read_parquet('my_data.parquet.gzip')\n",
    "parquet_mh_df.head()\n",
    "parquet_mh_df.equals(merged_mh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls_file = pd.ExcelFile('Inflation-data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_regex = re.compile(r'^\\d{4}$')\n",
    "anual_regex = re.compile(r'.*_a.*')\n",
    "page_list = []\n",
    "\n",
    "for page_name in xls_file.sheet_names:\n",
    "    if(anual_regex.match(page_name)):\n",
    "\n",
    "        df = pd.read_excel(xls_file, page_name, engine='openpyxl')\n",
    "        indicator = df['Series Name'][0]\n",
    "        print(indicator)\n",
    "        df = df.drop(columns=['IMF Country Code','Series Name','Indicator Type' ])\n",
    "        df = df[df['Country Code'].str.len() <= 3]\n",
    "        df = df[df['Country Code'].str.len() > 0]\n",
    "        melted_df = df.melt(id_vars=['Country Code', 'Country'], var_name='Year', value_name='Inflation')\n",
    "        melted_df = melted_df.rename(columns={'Inflation': indicator})\n",
    "        melted_df = melted_df[melted_df['Year'] != 'Note']\n",
    "        #melted_df = melted_df.drop(columns=['Series Name'])\n",
    "        \n",
    "        page_list.append(melted_df)\n",
    "#\n",
    "page_list[0].head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inflation = page_list[0]\n",
    "for i in range(1, len(page_list)):\n",
    "    merged_inflation = pd.merge(\n",
    "        merged_inflation,\n",
    "        page_list[i],\n",
    "        on=['Country', 'Year', 'Country Code'],\n",
    "        how='outer',\n",
    "        suffixes=('_left', '_right')\n",
    "    )\n",
    "\n",
    "merged_inflation = merged_inflation.loc[:, ~merged_inflation.columns.duplicated()]\n",
    "print(merged_inflation.isna().any())\n",
    "merged_inflation.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the numeric collumns to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of all the columns except the first 3 to float\n",
    "merged_inflation.iloc[:, 3:] = merged_inflation.iloc[:, 3:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_inflation.isna().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating missing values\n",
    "\n",
    "We use a simple inputer that fills the missing values on a collumn with the median value of that collumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inflation = fill_null_values(merged_inflation,'median')\n",
    "# check if any missing values remain in the float columns\n",
    "print(merged_inflation.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inflation = merged_inflation.dropna(axis=1, how='all')\n",
    "merged_inflation.to_csv('merged_inflation.csv', index=False)\n",
    "merged_inflation.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "before_2009 = pd.merge(merged_mh, merged_inflation, on=['Country', 'Country Code','Year'])\n",
    "after_2009 = pd.merge(before_2009,df_sm, on=['Year'])\n",
    "print(len(before_2009), len(after_2009))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_2009 = pd.merge(before_2009, df_income[['Country Code', 'incomeLevel']], on='Country Code', how='left', suffixes=('', '_income'))\n",
    "after_2009 = pd.merge(after_2009, df_income[['Country Code', 'incomeLevel']], on='Country Code', how='left', suffixes=('', '_income'))\n",
    "\n",
    "before_2009.head()\n",
    "after_2009.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "# Replace <username>, <password>, and <cluster_name> with your Atlas cluster details\n",
    "# You can find the connection string in the MongoDB Atlas dashboard\n",
    "connection_string = 'mongodb+srv://thatch:AmrXsPTlQtf4xnMP@bigdatacluster.bpvxx42.mongodb.net/'\n",
    "\n",
    "\n",
    "# Conectar ao servidor do MongoDB Atlas\n",
    "client = MongoClient(connection_string)\n",
    "\n",
    "# Selecionar o banco de dados e a coleção para armazenar os dados do Parquet\n",
    "db = client['BigData']\n",
    "collection1 = db['merged_before_2009_income']\n",
    "collection2 = db['merged_after_2009_income']\n",
    "\n",
    "# Converter os dados do Pandas em formato JSON\n",
    "records1 = before_2009.to_dict(orient='records')\n",
    "records2 = after_2009.to_dict(orient='records')\n",
    "\n",
    "# Inserir os dados na coleção do MongoDB\n",
    "collection1.insert_many(records1)\n",
    "collection2.insert_many(records2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
