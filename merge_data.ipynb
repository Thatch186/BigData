{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets Playbook \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sm = pd.read_csv('20230302 SocialMediaIndex.csv')\n",
    "df_sm.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental Health Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls_file = pd.ExcelFile('Mental health Depression disorder Data.xlsx')\n",
    "page_list = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each page on the excel sheet:\n",
    "- Load to a dataframe\n",
    "- Filter out rows with year different from 'yyyy' \n",
    "- Add the dataframe to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_regex = re.compile(r'^\\d{4}$')\n",
    "\n",
    "for page_name in xls_file.sheet_names:\n",
    "\n",
    "    df = pd.read_excel(xls_file, page_name, engine='openpyxl')\n",
    "    valid_years_mask = df['Year'].astype(str).apply(lambda x: bool(year_regex.match(x)))\n",
    "    filtered_df = df[valid_years_mask]\n",
    "\n",
    "    page_list.append(filtered_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the general dataset\n",
    "- Merging the different dataframes from each page by the columns 'Entity', 'Year', 'Code'\n",
    "- Removing columns with all values empty\n",
    "- removing duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = page_list[0]\n",
    "for i in range(1, len(page_list)):\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        page_list[i],\n",
    "        on=['Entity', 'Year', 'Code'],\n",
    "        how='outer',\n",
    "        suffixes=('_left', '_right')\n",
    "    )\n",
    "\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "merged_df = merged_df.dropna(axis=1, how='all')\n",
    "merged_df.to_csv('merged_dataset.csv', index=False)\n",
    "merged_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the resulting dataset to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use ParquetDataset to read in all of the files as a single dataset\n",
    "merged_df.to_parquet('my_data.parquet.gzip', compression='gzip')\n",
    "\n",
    "parquet_df = pd.read_parquet('my_data.parquet.gzip')\n",
    "parquet_df.head()\n",
    "parquet_df.equals(merged_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls_file = pd.ExcelFile('Inflation-data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_regex = re.compile(r'^\\d{4}$')\n",
    "anual_regex = re.compile(r'.*_a.*')\n",
    "page_list = []\n",
    "\n",
    "for page_name in xls_file.sheet_names:\n",
    "    if(anual_regex.match(page_name)):\n",
    "\n",
    "        df = pd.read_excel(xls_file, page_name, engine='openpyxl')\n",
    "        indicator = df['Series Name'][0]\n",
    "        print(indicator)\n",
    "        df = df.drop(columns=['IMF Country Code','Series Name','Indicator Type' ])\n",
    "        df = df[df['Country Code'].str.len() <= 3]\n",
    "        df = df[df['Country Code'].str.len() > 0]\n",
    "        melted_df = df.melt(id_vars=['Country Code', 'Country'], var_name='Year', value_name='Inflation')\n",
    "        melted_df = melted_df.rename(columns={'Inflation': indicator})\n",
    "        #melted_df = melted_df.drop(columns=['Series Name'])\n",
    "        \n",
    "        page_list.append(melted_df)\n",
    "\n",
    "page_list[0].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inflation = page_list[0]\n",
    "for i in range(1, len(page_list)):\n",
    "    merged_inflation = pd.merge(\n",
    "        merged_inflation,\n",
    "        page_list[i],\n",
    "        on=['Country', 'Year', 'Country Code'],\n",
    "        how='outer',\n",
    "        suffixes=('_left', '_right')\n",
    "    )\n",
    "\n",
    "merged_inflation = merged_inflation.loc[:, ~merged_inflation.columns.duplicated()]\n",
    "merged_inflation = merged_inflation.dropna(axis=1, how='all')\n",
    "merged_inflation.to_csv('merged_dataset_inflation.csv', index=False)\n",
    "merged_inflation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
