{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets Playbook \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_median_by_income_level(dataset):\n",
    "    # Get the columns to process (excluding the first three and last columns)\n",
    "    columns_to_process = dataset.columns[3:-1]\n",
    "\n",
    "    # Iterate over each column to process\n",
    "    for column in columns_to_process:\n",
    "        # Calculate the median for each incomeLevel group\n",
    "        median_by_income_level = dataset.groupby('incomeLevel')[column].transform('median')\n",
    "\n",
    "        # Check if there is at least one non-null value in each incomeLevel group\n",
    "        valid_groups = dataset.groupby('incomeLevel')[column].transform('count') > 0\n",
    "\n",
    "        # Fill null values with the median corresponding to the incomeLevel group if the group is valid\n",
    "        dataset[column] = np.where(valid_groups, dataset[column].fillna(median_by_income_level), 0)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wbgapi\n",
    "import wbgapi as wb\n",
    "\n",
    "economy = wb.economy.info()\n",
    "table_dict = vars(economy)\n",
    "df_economy = pd.DataFrame(table_dict.get('items'))\n",
    "\n",
    "filtered_dataset = df_economy.dropna(subset=['incomeLevel'])\n",
    "df_income = filtered_dataset[['id', 'incomeLevel']].copy()\n",
    "df_income.rename(columns={'id': 'Country Code'}, inplace=True)\n",
    "\n",
    "df_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_income['incomeLevel'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sm = pd.read_csv('social_media-ww-yearly-2009-2023.csv')\n",
    "df_sm = df_sm.rename(columns={'Date': 'Year'})\n",
    "df_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm = df_sm.astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental Health Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls_file = pd.ExcelFile('Mental health Depression disorder Data.xlsx')\n",
    "page_list = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each page on the excel sheet:\n",
    "- Load to a dataframe\n",
    "- Filter out rows with year different from 'yyyy' \n",
    "- Add the dataframe to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_regex = re.compile(r'^\\d{4}$')\n",
    "\n",
    "for page_name in xls_file.sheet_names:\n",
    "\n",
    "    df = pd.read_excel(xls_file, page_name, engine='openpyxl')\n",
    "    valid_years_mask = df['Year'].astype(str).apply(lambda x: bool(year_regex.match(x)))\n",
    "    filtered_df = df[valid_years_mask]\n",
    "\n",
    "    page_list.append(filtered_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the general dataset\n",
    "- Merging the different dataframes from each page by the columns 'Entity', 'Year', 'Code'\n",
    "- Removing columns with all values empty\n",
    "- removing duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mh = page_list[0]\n",
    "for i in range(1, len(page_list)):\n",
    "    merged_mh = pd.merge(\n",
    "        merged_mh,\n",
    "        page_list[i],\n",
    "        on=['Entity', 'Year', 'Code'],\n",
    "        how='outer',\n",
    "        suffixes=('_left', '_right')\n",
    "    )\n",
    "\n",
    "merged_mh = merged_mh.rename(columns={'Entity': 'Country', 'Code': 'Country Code'})\n",
    "merged_mh = merged_mh.loc[:, ~merged_mh.columns.duplicated()]\n",
    "merged_mh = merged_mh.dropna(axis=1, how='all')\n",
    "print(merged_mh.isna().any())\n",
    "merged_mh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of all the columns except the first 3 to float\n",
    "merged_mh.iloc[:, 3:] = merged_mh.iloc[:, 3:].astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Income Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mh = pd.merge(merged_mh, df_income[['Country Code', 'incomeLevel']], on='Country Code', how='left')\n",
    "merged_mh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_mh.shape)\n",
    "null_columns = merged_mh.columns[merged_mh.isnull().all()]\n",
    "\n",
    "# Step 2: Remove columns with all null values\n",
    "merged_mh = merged_mh.drop(null_columns, axis=1)\n",
    "print(merged_mh.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating missing values\n",
    "\n",
    "We use a simple inputer that fills the missing values on a collumn with the median value of that collumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mh = fill_median_by_income_level(merged_mh)\n",
    "# check if any missing values remain in the float columns\n",
    "print(merged_mh.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mh.to_csv('merged_health.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the resulting dataset to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use ParquetDataset to read in all of the files as a single dataset\n",
    "merged_mh.to_parquet('my_data.parquet.gzip', compression='gzip')\n",
    "\n",
    "parquet_mh_df = pd.read_parquet('my_data.parquet.gzip')\n",
    "parquet_mh_df.head()\n",
    "parquet_mh_df.equals(merged_mh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls_file = pd.ExcelFile('Inflation-data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_regex = re.compile(r'^\\d{4}$')\n",
    "anual_regex = re.compile(r'.*_a.*')\n",
    "page_list = []\n",
    "\n",
    "for page_name in xls_file.sheet_names:\n",
    "    if(anual_regex.match(page_name)):\n",
    "\n",
    "        df = pd.read_excel(xls_file, page_name, engine='openpyxl')\n",
    "        indicator = df['Series Name'][0]\n",
    "        print(indicator)\n",
    "        df = df.drop(columns=['IMF Country Code','Series Name','Indicator Type' ])\n",
    "        df = df[df['Country Code'].str.len() <= 3]\n",
    "        df = df[df['Country Code'].str.len() > 0]\n",
    "        melted_df = df.melt(id_vars=['Country Code', 'Country'], var_name='Year', value_name='Inflation')\n",
    "        melted_df = melted_df.rename(columns={'Inflation': indicator})\n",
    "        melted_df = melted_df[melted_df['Year'] != 'Note']\n",
    "        #melted_df = melted_df.drop(columns=['Series Name'])\n",
    "        \n",
    "        page_list.append(melted_df)\n",
    "#\n",
    "page_list[0].head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inflation = page_list[0]\n",
    "for i in range(1, len(page_list)):\n",
    "    merged_inflation = pd.merge(\n",
    "        merged_inflation,\n",
    "        page_list[i],\n",
    "        on=['Country', 'Year', 'Country Code'],\n",
    "        how='outer',\n",
    "        suffixes=('_left', '_right')\n",
    "    )\n",
    "\n",
    "merged_inflation = merged_inflation.loc[:, ~merged_inflation.columns.duplicated()]\n",
    "print(merged_inflation.isna().any())\n",
    "merged_inflation.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the numeric collumns to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of all the columns except the first 3 to float\n",
    "merged_inflation.iloc[:, 3:] = merged_inflation.iloc[:, 3:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_inflation.isna().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Income Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inflation = pd.merge(merged_inflation, df_income[['Country Code', 'incomeLevel']], on='Country Code', how='left')\n",
    "merged_inflation.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating missing values\n",
    "\n",
    "We use a simple inputer that fills the missing values on a collumn with the median value of that collumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inflation = fill_median_by_income_level(merged_inflation)\n",
    "# check if any missing values remain in the float columns\n",
    "print(merged_inflation.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inflation = merged_inflation.dropna(axis=1, how='all')\n",
    "merged_inflation.to_csv('merged_inflation.csv', index=False)\n",
    "merged_inflation.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "before_2009 = pd.merge(merged_mh, merged_inflation, on=['Country', 'Country Code','Year'])\n",
    "after_2009 = pd.merge(before_2009,df_sm, on=['Year'])\n",
    "print(len(before_2009), len(after_2009))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_2009 = pd.merge(before_2009, df_income[['Country Code', 'incomeLevel']], on='Country Code', how='left', suffixes=('', '_y'))\n",
    "before_2009 = before_2009.drop(columns=['incomeLevel_y'])\n",
    "after_2009 = pd.merge(after_2009, df_income[['Country Code', 'incomeLevel']], on='Country Code', how='left')\n",
    "\n",
    "before_2009.head()\n",
    "after_2009.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "# Replace <username>, <password>, and <cluster_name> with your Atlas cluster details\n",
    "# You can find the connection string in the MongoDB Atlas dashboard\n",
    "connection_string = 'mongodb+srv://thatch:AmrXsPTlQtf4xnMP@bigdatacluster.bpvxx42.mongodb.net/'\n",
    "\n",
    "\n",
    "# Conectar ao servidor do MongoDB Atlas\n",
    "client = MongoClient(connection_string)\n",
    "\n",
    "# Selecionar o banco de dados e a coleção para armazenar os dados do Parquet\n",
    "db = client['BigData']\n",
    "collection1 = db['before_2009']\n",
    "collection2 = db['after_2009']\n",
    "\n",
    "# Precisamos de fazer este upload pois os valores médios das redes sociais estavam a ser afetados pelos valores em falta\n",
    "sm_collection = db['social_media']\n",
    "# Converter os dados do Pandas em formato JSON\n",
    "records1 = before_2009.to_dict(orient='records')\n",
    "records2 = after_2009.to_dict(orient='records')\n",
    "records_sm = df_sm.to_dict(orient='records')\n",
    "# Inserir os dados na coleção do MongoDB\n",
    "collection1.insert_many(records1)\n",
    "collection2.insert_many(records2)\n",
    "sm_collection.insert_many(records_sm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
